{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Member 2 Feature Engineering (Signal Construction)\n",
    "\n",
    "**Goal:** Turn company attributes into **numeric, comparable, explainable signals** for PCA + clustering.\n",
    "\n",
    "**Scope:** Start from Member 1's cleaned dataset and create features without global cleaning decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 250)\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "if ROOT.name.lower() == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "DATA_DIR = ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "SOURCE_PATH = PROCESSED_DIR / \"cleaned_base.csv\"\n",
    "\n",
    "OUTPUT_DIR = PROCESSED_DIR\n",
    "DOCS_DIR = ROOT / \"docs\"\n",
    "\n",
    "CURRENT_YEAR = 2026\n",
    "EXPORT_FEATURES = False\n",
    "PLOT_FEATURE_OVERVIEW = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Load the cleaned base dataset produced by Member 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not SOURCE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing source file: {SOURCE_PATH}\")\n",
    "\n",
    "df = pd.read_csv(SOURCE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "Apply lightweight, non-destructive cleanup to prepare for feature construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "df.columns = df.columns.str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Helpers\n",
    "Reusable helpers for parsing, normalization, and feature construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "def has_value(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(\"string\").str.strip()\n",
    "    return s.notna() & s.ne(\"\")\n",
    "\n",
    "def normalize_text(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(\"string\").str.strip().str.upper()\n",
    "    return s.replace({\"\": pd.NA})\n",
    "\n",
    "def range_to_midpoint(value):\n",
    "    if isinstance(value, str):\n",
    "        if \"to\" in value:\n",
    "            parts = value.split(\"to\")\n",
    "        elif \"-\" in value:\n",
    "            parts = value.split(\"-\")\n",
    "        else:\n",
    "            return np.nan\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                low = float(parts[0].strip())\n",
    "                high = float(parts[1].strip())\n",
    "                return (low + high) / 2\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "    return np.nan\n",
    "\n",
    "_split_re = re.compile(r\"[;,|]+\")\n",
    "\n",
    "def count_delimited_items(value) -> int:\n",
    "    if value is None:\n",
    "        return 0\n",
    "    if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "    s = value.strip()\n",
    "    if s == \"\":\n",
    "        return 0\n",
    "    parts = [p.strip() for p in _split_re.split(s) if p.strip()]\n",
    "    return len(parts) if parts else 0\n",
    "\n",
    "def safe_to_numeric(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def add_numeric_feature(features: pd.DataFrame, series: pd.Series, name: str, log: bool = True):\n",
    "    series = safe_to_numeric(series)\n",
    "    missing = series.isna()\n",
    "    features[f\"{name}_missing\"] = missing.astype(int)\n",
    "    if series.notna().any():\n",
    "        fill_value = series.median()\n",
    "        series_filled = series.fillna(fill_value)\n",
    "    else:\n",
    "        series_filled = series.fillna(0)\n",
    "    features[name] = series_filled\n",
    "    if log:\n",
    "        features[f\"log_{name}\"] = np.log1p(series_filled.clip(lower=0))\n",
    "\n",
    "def missing_ratio_for(df: pd.DataFrame, cols):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    if not cols:\n",
    "        return pd.Series(0, index=df.index)\n",
    "    present = {}\n",
    "    for c in cols:\n",
    "        s = df[c]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            present[c] = s.notna()\n",
    "        else:\n",
    "            present[c] = has_value(s)\n",
    "    present_df = pd.DataFrame(present)\n",
    "    return 1 - present_df.mean(axis=1)\n",
    "\n",
    "def parse_range_or_numeric(value):\n",
    "    if isinstance(value, (int, float)) and not pd.isna(value):\n",
    "        return float(value)\n",
    "    if isinstance(value, str):\n",
    "        mid = range_to_midpoint(value)\n",
    "        if not pd.isna(mid):\n",
    "            return mid\n",
    "        try:\n",
    "            return float(value.strip())\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "def add_has(df: pd.DataFrame, features: pd.DataFrame, col: str, feature_name: str):\n",
    "    if col in df.columns:\n",
    "        features[feature_name] = has_value(df[col]).astype(int)\n",
    "\n",
    "_bool_map = {\"true\": 1, \"false\": 0, \"yes\": 1, \"no\": 0, \"y\": 1, \"n\": 0, \"1\": 1, \"0\": 0}\n",
    "\n",
    "def add_boolean_feature(df: pd.DataFrame, features: pd.DataFrame, col: str, name: str):\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    s = df[col].astype(\"string\").str.strip().str.lower()\n",
    "    mapped = s.map(_bool_map)\n",
    "    features[name] = mapped.fillna(0).astype(int)\n",
    "    features[f\"{name}_missing\"] = mapped.isna().astype(int)\n",
    "\n",
    "def find_first_column(df: pd.DataFrame, candidates):\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Transparency and Structure Signals\n",
    "Create low-risk, interpretable indicators for traceability and entity structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "features = pd.DataFrame(index=df.index)\n",
    "\n",
    "# Transparency / traceability\n",
    "add_has(df, features, \"Website\", \"has_website\")\n",
    "add_has(df, features, \"Phone Number\", \"has_phone\")\n",
    "add_has(df, features, \"Address Line 1\", \"has_address\")\n",
    "add_has(df, features, \"City\", \"has_city\")\n",
    "add_has(df, features, \"State\", \"has_state\")\n",
    "add_has(df, features, \"State Or Province Abbreviation\", \"has_state_abbrev\")\n",
    "add_has(df, features, \"Postal Code\", \"has_postal_code\")\n",
    "add_has(df, features, \"Country\", \"has_country\")\n",
    "add_has(df, features, \"Region\", \"has_region\")\n",
    "\n",
    "# Company name (column appears labeled as Company Sites in this dataset)\n",
    "add_has(df, features, \"Company Sites\", \"has_company_name\")\n",
    "\n",
    "# Ownership / structure\n",
    "add_has(df, features, \"Parent Company\", \"has_parent\")\n",
    "add_has(df, features, \"Global Ultimate Company\", \"has_global_ultimate\")\n",
    "add_has(df, features, \"Domestic Ultimate Company\", \"has_domestic_ultimate\")\n",
    "\n",
    "# Verifiability extras\n",
    "add_has(df, features, \"Ticker\", \"has_ticker\")\n",
    "add_has(df, features, \"Registration Number\", \"has_registration_number\")\n",
    "add_has(df, features, \"Company Description\", \"has_company_description\")\n",
    "add_has(df, features, \"Legal Status\", \"has_legal_status\")\n",
    "add_has(df, features, \"Ownership Type\", \"has_ownership_type\")\n",
    "add_has(df, features, \"Entity Type\", \"has_entity_type\")\n",
    "\n",
    "# Company status (value-coded)\n",
    "status_col = \"Company Status (Active/Inactive)\"\n",
    "if status_col in df.columns:\n",
    "    status = df[status_col].astype(\"string\").str.strip().str.lower()\n",
    "    status_map = {\"active\": 1, \"inactive\": 0}\n",
    "    features[\"company_status_binary\"] = status.map(status_map)\n",
    "    features[\"has_company_status\"] = features[\"company_status_binary\"].notna().astype(int)\n",
    "    features[\"company_status_binary\"] = features[\"company_status_binary\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credibility and Missingness Signals\n",
    "Summarize completeness of key fields and compute group-wise missingness ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "credibility_flag_cols = [c for c in [\n",
    "    \"has_website\", \"has_address\", \"has_phone\",\n",
    "    \"has_ticker\", \"has_parent\", \"has_global_ultimate\", \"has_domestic_ultimate\",\n",
    "    \"has_registration_number\", \"has_company_description\",\n",
    "    \"has_company_status\"\n",
    "] if c in features.columns]\n",
    "\n",
    "if credibility_flag_cols:\n",
    "    features[\"credibility_score\"] = features[credibility_flag_cols].sum(axis=1)\n",
    "    features[\"credibility_score_norm\"] = features[\"credibility_score\"] / len(credibility_flag_cols)\n",
    "    features[\"missing_ratio_credibility\"] = 1 - features[\"credibility_score_norm\"]\n",
    "\n",
    "contact_cols = [\n",
    "    \"Website\", \"Phone Number\", \"Address Line 1\", \"City\", \"State\",\n",
    "    \"State Or Province Abbreviation\", \"Postal Code\", \"Country\", \"Region\"\n",
    "]\n",
    "ownership_cols = [\n",
    "    \"Parent Company\", \"Parent Country/Region\",\n",
    "    \"Global Ultimate Company\", \"Global Ultimate Country Name\",\n",
    "    \"Domestic Ultimate Company\"\n",
    "]\n",
    "financial_cols = [\n",
    "    \"Employees Single Site\", \"Employees Total\", \"Revenue (USD)\", \"Market Value (USD)\",\n",
    "    \"Corporate Family Members\", \"Year Found\"\n",
    "]\n",
    "it_cols = [\n",
    "    \"No. of PC\", \"No. of Desktops\", \"No. of Laptops\", \"No. of Routers\",\n",
    "    \"No. of Servers\", \"No. of Storage Devices\", \"IT Budget\", \"IT spend\"\n",
    "]\n",
    "code_cols = [\n",
    "    \"SIC Code\", \"8-Digit SIC Code\", \"NAICS Code\", \"NACE Rev 2 Code\",\n",
    "    \"ANZSIC Code\", \"ISIC Rev 4 Code\"\n",
    "]\n",
    "\n",
    "features[\"missing_ratio_contact\"] = missing_ratio_for(df, contact_cols)\n",
    "features[\"missing_ratio_ownership\"] = missing_ratio_for(df, ownership_cols)\n",
    "features[\"missing_ratio_financial\"] = missing_ratio_for(df, financial_cols)\n",
    "features[\"missing_ratio_it\"] = missing_ratio_for(df, it_cols)\n",
    "features[\"missing_ratio_codes\"] = missing_ratio_for(df, code_cols)\n",
    "\n",
    "all_missing_cols = contact_cols + ownership_cols + financial_cols + it_cols + code_cols\n",
    "features[\"missing_ratio_overall\"] = missing_ratio_for(df, all_missing_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizational Complexity\n",
    "Estimate group size based on global ultimate ownership.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if \"Global Ultimate Company\" in df.columns:\n",
    "    key = normalize_text(df[\"Global Ultimate Company\"])\n",
    "    group_sizes = key.groupby(key).transform(\"size\")\n",
    "    features[\"org_complexity_count\"] = group_sizes.fillna(0).astype(int)\n",
    "    features[\"log_org_complexity_count\"] = np.log1p(features[\"org_complexity_count\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale and Market Signals\n",
    "Normalize size metrics and compute scale-related ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "numeric_cols = {\n",
    "    \"Employees Total\": \"employees_total\",\n",
    "    \"Employees Single Site\": \"employees_single_site\",\n",
    "    \"Revenue (USD)\": \"revenue_usd\",\n",
    "    \"Market Value (USD)\": \"market_value_usd\",\n",
    "    \"Corporate Family Members\": \"corporate_family_members\",\n",
    "}\n",
    "\n",
    "for col, name in numeric_cols.items():\n",
    "    if col in df.columns:\n",
    "        add_numeric_feature(features, df[col], name, log=True)\n",
    "\n",
    "if \"Year Found\" in df.columns:\n",
    "    year_found = safe_to_numeric(df[\"Year Found\"])\n",
    "    company_age = CURRENT_YEAR - year_found\n",
    "    company_age = company_age.where((company_age >= 0) & (company_age <= 300))\n",
    "    features[\"company_age_missing\"] = company_age.isna().astype(int)\n",
    "    fill_value = company_age.median()\n",
    "    if pd.isna(fill_value):\n",
    "        fill_value = 0\n",
    "    company_age_filled = company_age.fillna(fill_value)\n",
    "    features[\"company_age\"] = company_age_filled\n",
    "    features[\"log_company_age\"] = np.log1p(company_age_filled.clip(lower=0))\n",
    "\n",
    "if \"employees_total\" in features.columns and \"employees_single_site\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"employee_concentration\"] = (features[\"employees_single_site\"] / denom).fillna(0)\n",
    "\n",
    "if \"revenue_usd\" in features.columns and \"employees_total\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"revenue_per_employee\"] = (features[\"revenue_usd\"] / denom).fillna(0)\n",
    "\n",
    "if \"market_value_usd\" in features.columns and \"employees_total\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"market_value_per_employee\"] = (features[\"market_value_usd\"] / denom).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geography and Multinational Heuristics\n",
    "Derive country-level comparisons and multinational indicators.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if \"Lattitude\" in df.columns:\n",
    "    add_numeric_feature(features, df[\"Lattitude\"], \"latitude\", log=False)\n",
    "\n",
    "if \"Longitude\" in df.columns:\n",
    "    add_numeric_feature(features, df[\"Longitude\"], \"longitude\", log=False)\n",
    "\n",
    "entity_country = normalize_text(df[\"Country\"]) if \"Country\" in df.columns else pd.Series(pd.NA, index=df.index)\n",
    "parent_country = normalize_text(df[\"Parent Country/Region\"]) if \"Parent Country/Region\" in df.columns else pd.Series(pd.NA, index=df.index)\n",
    "global_country = normalize_text(df[\"Global Ultimate Country Name\"]) if \"Global Ultimate Country Name\" in df.columns else pd.Series(pd.NA, index=df.index)\n",
    "\n",
    "parent_present = has_value(df[\"Parent Company\"]) if \"Parent Company\" in df.columns else pd.Series(False, index=df.index)\n",
    "global_present = has_value(df[\"Global Ultimate Company\"]) if \"Global Ultimate Company\" in df.columns else pd.Series(False, index=df.index)\n",
    "\n",
    "features[\"parent_foreign_flag\"] = (parent_present & parent_country.notna() & (parent_country != entity_country)).astype(int)\n",
    "features[\"global_ultimate_foreign_flag\"] = (global_present & global_country.notna() & (global_country != entity_country)).astype(int)\n",
    "features[\"multinational_flag\"] = ((features[\"parent_foreign_flag\"] == 1) | (features[\"global_ultimate_foreign_flag\"] == 1)).astype(int)\n",
    "\n",
    "countries_df = pd.concat([entity_country, parent_country, global_country], axis=1)\n",
    "features[\"num_countries_reported\"] = countries_df.nunique(axis=1, dropna=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IT and Operational Footprint Signals\n",
    "Parse IT asset ranges and create intensity ratios.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "asset_cols_map = {\n",
    "    \"No. of PC\": \"pc_midpoint\",\n",
    "    \"No. of Desktops\": \"desktops_midpoint\",\n",
    "    \"No. of Laptops\": \"laptops_midpoint\",\n",
    "    \"No. of Routers\": \"routers_midpoint\",\n",
    "    \"No. of Servers\": \"servers_midpoint\",\n",
    "    \"No. of Storage Devices\": \"storage_devices_midpoint\",\n",
    "}\n",
    "\n",
    "for col, name in asset_cols_map.items():\n",
    "    if col in df.columns:\n",
    "        series = df[col].apply(parse_range_or_numeric)\n",
    "        add_numeric_feature(features, series, name, log=True)\n",
    "\n",
    "asset_feature_cols = [name for name in asset_cols_map.values() if name in features.columns]\n",
    "if asset_feature_cols:\n",
    "    features[\"it_assets_total\"] = features[asset_feature_cols].sum(axis=1)\n",
    "    features[\"log_it_assets_total\"] = np.log1p(features[\"it_assets_total\"])\n",
    "\n",
    "if \"IT Budget\" in df.columns:\n",
    "    add_numeric_feature(features, df[\"IT Budget\"], \"it_budget\", log=True)\n",
    "if \"IT spend\" in df.columns:\n",
    "    add_numeric_feature(features, df[\"IT spend\"], \"it_spend\", log=True)\n",
    "\n",
    "if \"it_budget\" in features.columns and \"it_spend\" in features.columns:\n",
    "    denom = features[\"it_budget\"].replace(0, np.nan)\n",
    "    features[\"it_spend_rate\"] = (features[\"it_spend\"] / denom).fillna(0).clip(lower=0, upper=3)\n",
    "    features[\"it_budget_gap\"] = features[\"it_budget\"] - features[\"it_spend\"]\n",
    "    features[\"log_abs_it_budget_gap\"] = np.log1p(features[\"it_budget_gap\"].abs())\n",
    "\n",
    "if \"it_assets_total\" in features.columns and \"employees_total\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"it_assets_per_employee\"] = (features[\"it_assets_total\"] / denom).fillna(0)\n",
    "\n",
    "if \"it_spend\" in features.columns and \"employees_total\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"it_spend_per_employee\"] = (features[\"it_spend\"] / denom).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industry Code Features\n",
    "Convert high-cardinality industry codes into compact counts and buckets.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "code_cols = {\n",
    "    \"SIC Code\": \"sic_code\",\n",
    "    \"8-Digit SIC Code\": \"sic8_code\",\n",
    "    \"NAICS Code\": \"naics_code\",\n",
    "    \"NACE Rev 2 Code\": \"nace2_code\",\n",
    "    \"ANZSIC Code\": \"anzsic_code\",\n",
    "    \"ISIC Rev 4 Code\": \"isic4_code\",\n",
    "}\n",
    "\n",
    "for col, prefix in code_cols.items():\n",
    "    if col in df.columns:\n",
    "        counts = df[col].astype(\"string\").apply(count_delimited_items)\n",
    "        features[f\"{prefix}_count\"] = counts\n",
    "\n",
    "        codes = df[col].astype(\"string\").str.strip()\n",
    "        sector = codes.str[:2].replace({\"\": pd.NA})\n",
    "        sector_dummies = pd.get_dummies(sector, prefix=f\"{prefix}_sector\", dummy_na=True)\n",
    "        features = pd.concat([features, sector_dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Encoding\n",
    "Create low-cardinality one-hot features and boolean flags.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "add_boolean_feature(df, features, \"Is Headquarters\", \"is_headquarters\")\n",
    "add_boolean_feature(df, features, \"Is Domestic Ultimate\", \"is_domestic_ultimate\")\n",
    "\n",
    "candidate_categoricals = [\n",
    "    \"Region\",\n",
    "    \"Entity Type\",\n",
    "    \"Ownership Type\",\n",
    "    \"Legal Status\",\n",
    "    \"Franchise Status\",\n",
    "    \"Manufacturing Status\",\n",
    "    \"Registration Number Type\",\n",
    "]\n",
    "\n",
    "categorical_cols = []\n",
    "for col in candidate_categoricals:\n",
    "    if col in df.columns and df[col].nunique(dropna=True) <= 20:\n",
    "        categorical_cols.append(col)\n",
    "\n",
    "if categorical_cols:\n",
    "    df_cats = df[categorical_cols].copy()\n",
    "    for col in categorical_cols:\n",
    "        df_cats[col] = df_cats[col].astype(\"string\").str.strip()\n",
    "    df_dummies = pd.get_dummies(df_cats, prefix=categorical_cols, prefix_sep=\"__\", dummy_na=True)\n",
    "else:\n",
    "    df_dummies = pd.DataFrame(index=df.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Feature Matrices\n",
    "Combine engineered features and one-hot encodings into raw and numeric-only matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_features_raw = pd.concat([features, df_dummies], axis=1)\n",
    "df_features_raw = df_features_raw.dropna(axis=1, how=\"all\")\n",
    "df_features_raw = df_features_raw.apply(pd.to_numeric, errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Remaining NaNs and Scale\n",
    "Fill any remaining missing values and build a scaled matrix for PCA/clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if df_features_raw.isna().any().any():\n",
    "    nan_cols = df_features_raw.columns[df_features_raw.isna().any()].tolist()\n",
    "    for col in nan_cols:\n",
    "        series = df_features_raw[col]\n",
    "        fill_value = series.median()\n",
    "        if pd.isna(fill_value):\n",
    "            fill_value = 0\n",
    "        df_features_raw[col] = series.fillna(fill_value)\n",
    "\n",
    "assert not df_features_raw.isna().any().any(), \"NaNs remain in raw features\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_features_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_features_raw),\n",
    "    columns=df_features_raw.columns,\n",
    "    index=df_features_raw.index,\n",
    ")\n",
    "\n",
    "assert not df_features_scaled.isna().any().any(), \"NaNs remain in scaled features\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row ID Map\n",
    "Create a lookup table to map features back to source identifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "duns_col = None\n",
    "for col in df.columns:\n",
    "    if col.strip().lower() == \"duns number\":\n",
    "        duns_col = col\n",
    "        break\n",
    "\n",
    "name_col = find_first_column(df, [\"Company Name\", \"Company\", \"Company Sites\"])\n",
    "\n",
    "row_id_map = pd.DataFrame({\"row_index\": df.index})\n",
    "if duns_col:\n",
    "    row_id_map[\"DUNS Number\"] = df[duns_col]\n",
    "if name_col:\n",
    "    row_id_map[\"Company Name\"] = df[name_col]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Dictionary\n",
    "Document feature meanings for downstream consumers.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_descriptions = {\n",
    "    \"has_website\": \"1 if Website is present\",\n",
    "    \"has_phone\": \"1 if Phone Number is present\",\n",
    "    \"has_address\": \"1 if Address Line 1 is present\",\n",
    "    \"has_city\": \"1 if City is present\",\n",
    "    \"has_state\": \"1 if State is present\",\n",
    "    \"has_state_abbrev\": \"1 if State Or Province Abbreviation is present\",\n",
    "    \"has_postal_code\": \"1 if Postal Code is present\",\n",
    "    \"has_country\": \"1 if Country is present\",\n",
    "    \"has_region\": \"1 if Region is present\",\n",
    "    \"has_company_name\": \"1 if Company name is present\",\n",
    "    \"has_parent\": \"1 if Parent Company is present\",\n",
    "    \"has_global_ultimate\": \"1 if Global Ultimate Company is present\",\n",
    "    \"has_domestic_ultimate\": \"1 if Domestic Ultimate Company is present\",\n",
    "    \"has_ticker\": \"1 if Ticker is present\",\n",
    "    \"has_registration_number\": \"1 if Registration Number is present\",\n",
    "    \"has_company_description\": \"1 if Company Description is present\",\n",
    "    \"has_legal_status\": \"1 if Legal Status is present\",\n",
    "    \"has_ownership_type\": \"1 if Ownership Type is present\",\n",
    "    \"has_entity_type\": \"1 if Entity Type is present\",\n",
    "    \"company_status_binary\": \"1 if Company Status is Active; 0 otherwise\",\n",
    "    \"credibility_score\": \"Count of key transparency signals\",\n",
    "    \"credibility_score_norm\": \"Credibility score normalized to [0,1]\",\n",
    "    \"missing_ratio_credibility\": \"1 - normalized credibility score\",\n",
    "    \"missing_ratio_contact\": \"Missing ratio for contact fields\",\n",
    "    \"missing_ratio_ownership\": \"Missing ratio for ownership fields\",\n",
    "    \"missing_ratio_financial\": \"Missing ratio for financial fields\",\n",
    "    \"missing_ratio_it\": \"Missing ratio for IT fields\",\n",
    "    \"missing_ratio_codes\": \"Missing ratio for code fields\",\n",
    "    \"missing_ratio_overall\": \"Missing ratio across all key fields\",\n",
    "    \"org_complexity_count\": \"Number of companies in same global ultimate group\",\n",
    "    \"log_org_complexity_count\": \"Log of org complexity count\",\n",
    "    \"employees_total\": \"Employees Total (filled)\",\n",
    "    \"employees_single_site\": \"Employees Single Site (filled)\",\n",
    "    \"revenue_usd\": \"Revenue in USD (filled)\",\n",
    "    \"market_value_usd\": \"Market Value in USD (filled)\",\n",
    "    \"corporate_family_members\": \"Corporate Family Members (filled)\",\n",
    "    \"company_age\": \"Company age in years\",\n",
    "    \"employee_concentration\": \"Employees single site / total employees\",\n",
    "    \"revenue_per_employee\": \"Revenue per employee\",\n",
    "    \"market_value_per_employee\": \"Market value per employee\",\n",
    "    \"latitude\": \"Latitude (filled)\",\n",
    "    \"longitude\": \"Longitude (filled)\",\n",
    "    \"parent_foreign_flag\": \"1 if parent country differs from entity country\",\n",
    "    \"global_ultimate_foreign_flag\": \"1 if global ultimate country differs from entity country\",\n",
    "    \"multinational_flag\": \"1 if any parent/ultimate is foreign\",\n",
    "    \"num_countries_reported\": \"Count of unique countries reported\",\n",
    "    \"it_assets_total\": \"Total IT assets midpoint sum\",\n",
    "    \"it_budget\": \"IT budget (filled)\",\n",
    "    \"it_spend\": \"IT spend (filled)\",\n",
    "    \"it_spend_rate\": \"IT spend / IT budget\",\n",
    "    \"it_budget_gap\": \"IT budget minus IT spend\",\n",
    "    \"it_assets_per_employee\": \"IT assets per employee\",\n",
    "    \"it_spend_per_employee\": \"IT spend per employee\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Column Pruning\n",
    "Remove near-duplicate or low-signal columns prior to downstream modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    \"has_state_abbrev\",\n",
    "    \"company_status_binary\",\n",
    "    \"credibility_score\",\n",
    "    \"missing_ratio_credibility\",\n",
    "    \"org_complexity_count\",\n",
    "    \"employees_total\",\n",
    "    \"log_employees_single_site\",\n",
    "    \"revenue_usd\",\n",
    "    \"market_value_usd\",\n",
    "    \"corporate_family_members\",\n",
    "    \"company_age\",\n",
    "    \"pc_midpoint\",\n",
    "    \"desktops_midpoint\",\n",
    "    \"laptops_midpoint\",\n",
    "    \"routers_midpoint\",\n",
    "    \"servers_midpoint\",\n",
    "    \"storage_devices_midpoint\",\n",
    "    \"it_assets_total\",\n",
    "    \"it_budget\",\n",
    "    \"it_spend\",\n",
    "    \"it_budget_gap\",\n",
    "]\n",
    "\n",
    "df_features_scaled_drop = df_features_scaled.drop(columns=cols_to_drop, errors=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Artifacts (Optional)\n",
    "Persist feature matrices and documentation for downstream modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if EXPORT_FEATURES:\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    features_raw_path = OUTPUT_DIR / \"features_for_clustering_raw.csv\"\n",
    "    features_scaled_path = OUTPUT_DIR / \"features_for_clustering_scaled.csv\"\n",
    "    row_id_map_path = OUTPUT_DIR / \"row_id_map.csv\"\n",
    "    feature_dict_path = DOCS_DIR / \"feature_dictionary_member2.csv\"\n",
    "\n",
    "    df_features_raw.to_csv(features_raw_path, index=False)\n",
    "    df_features_scaled.to_csv(features_scaled_path, index=False)\n",
    "    row_id_map.to_csv(row_id_map_path, index=False)\n",
    "    pd.DataFrame(sorted(feature_descriptions.items()), columns=[\"feature\", \"description\"]).to_csv(\n",
    "        feature_dict_path, index=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Quick optional overview plots for feature completeness.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "if PLOT_FEATURE_OVERVIEW:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    missing_rates = df_features_raw.isna().mean().sort_values(ascending=False).head(25)\n",
    "    if not missing_rates.empty:\n",
    "        ax = missing_rates.plot(kind=\"bar\", figsize=(12, 4))\n",
    "        ax.set_title(\"Top 25 Feature Missing Rates\")\n",
    "        ax.set_ylabel(\"Missing Rate\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}