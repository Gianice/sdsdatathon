{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63f0b0fb",
   "metadata": {},
   "source": [
    "# Member 2 â€” Feature Engineering (Signal Construction)\n",
    "\n",
    "**Goal:** Turn company attributes into **numeric, comparable, explainable signals** for **PCA + clustering**.\n",
    "\n",
    "**Scope (Member 2):**\n",
    "- Start from **Member 1's cleaned & imputed dataset** (recommended: `data/processed/clean_base.csv`).\n",
    "- Do **not** make global cleaning decisions (dropping rows/columns, global imputation strategies, etc.).\n",
    "- You *may* create **presence indicators** and derived features that treat missingness as signal.\n",
    "\n",
    "**Outputs (for handoff):**\n",
    "- `df_features_raw`: numeric feature matrix (not scaled)\n",
    "- `df_features_scaled`: scaled matrix for PCA/clustering\n",
    "- `feature_dict`: short description of engineered features\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580538a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', 250)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a83d3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_base shape: (8559, 72)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_53240\\2524596087.py:8: SyntaxWarning: invalid escape sequence '\\p'\n",
      "  SOURCE_PATH = ROOT / \"data\\processed\" / \"cleaned_base.csv\"\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_53240\\2524596087.py:16: DtypeWarning: Columns (49) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(SOURCE_PATH)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# Copy clean_base and load\n",
    "# -------------------------------\n",
    "ROOT = Path.cwd()\n",
    "if ROOT.name.lower() == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "SOURCE_PATH = ROOT / \"data\\processed\" / \"cleaned_base.csv\"\n",
    "DEST_DIR = ROOT / \"data\" / \"processed\"\n",
    "DEST_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not SOURCE_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing source file: {SOURCE_PATH}\")\n",
    "\n",
    "\n",
    "df = pd.read_csv(SOURCE_PATH)\n",
    "\n",
    "print(\"clean_base shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6362f",
   "metadata": {},
   "source": [
    "## 1) Helper functions\n",
    "\n",
    "We standardize \"presence\" checks so empty strings don't count as filled.\n",
    "We also parse range strings like `\"1 to 10\"` into numeric midpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f63b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_value(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(\"string\").str.strip()\n",
    "    return s.notna() & s.ne(\"\")\n",
    "\n",
    "def normalize_text(series: pd.Series) -> pd.Series:\n",
    "    s = series.astype(\"string\").str.strip().str.upper()\n",
    "    return s.replace({\"\": pd.NA})\n",
    "\n",
    "def range_to_midpoint(value):\n",
    "    if isinstance(value, str):\n",
    "        if \"to\" in value:\n",
    "            parts = value.split(\"to\")\n",
    "        elif \"-\" in value:\n",
    "            parts = value.split(\"-\")\n",
    "        else:\n",
    "            return np.nan\n",
    "        if len(parts) >= 2:\n",
    "            try:\n",
    "                low = float(parts[0].strip())\n",
    "                high = float(parts[1].strip())\n",
    "                return (low + high) / 2\n",
    "            except ValueError:\n",
    "                return np.nan\n",
    "    return np.nan\n",
    "\n",
    "_split_re = re.compile(r\"[;,|]+\")\n",
    "\n",
    "def count_delimited_items(value) -> int:\n",
    "    if value is None:\n",
    "        return 0\n",
    "    if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "    s = value.strip()\n",
    "    if s == \"\":\n",
    "        return 0\n",
    "    parts = [p.strip() for p in _split_re.split(s) if p.strip()]\n",
    "    return len(parts) if parts else 0\n",
    "\n",
    "def safe_to_numeric(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(series, errors=\"coerce\")\n",
    "\n",
    "def add_numeric_feature(features: pd.DataFrame, series: pd.Series, name: str, log: bool = True):\n",
    "    series = safe_to_numeric(series)\n",
    "    missing = series.isna()\n",
    "    features[f\"{name}_missing\"] = missing.astype(int)\n",
    "    if series.notna().any():\n",
    "        fill_value = series.median()\n",
    "        series_filled = series.fillna(fill_value)\n",
    "    else:\n",
    "        series_filled = series.fillna(0)\n",
    "    features[name] = series_filled\n",
    "    if log:\n",
    "        features[f\"log_{name}\"] = np.log1p(series_filled.clip(lower=0))\n",
    "\n",
    "def missing_ratio_for(cols):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    if not cols:\n",
    "        return pd.Series(0, index=df.index)\n",
    "    present = {}\n",
    "    for c in cols:\n",
    "        s = df[c]\n",
    "        if pd.api.types.is_numeric_dtype(s):\n",
    "            present[c] = s.notna()\n",
    "        else:\n",
    "            present[c] = has_value(s)\n",
    "    present_df = pd.DataFrame(present)\n",
    "    return 1 - present_df.mean(axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d30dc",
   "metadata": {},
   "source": [
    "## 2) Core transparency + structure signals (curated short names)\n",
    "\n",
    "These are your **interpretable, low-risk** binary signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6667ed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.DataFrame(index=df.index)\n",
    "\n",
    "def add_has(col, feature_name):\n",
    "    if col in df.columns:\n",
    "        features[feature_name] = has_value(df[col]).astype(int)\n",
    "\n",
    "# Transparency / traceability\n",
    "add_has(\"Website\", \"has_website\")\n",
    "add_has(\"Phone Number\", \"has_phone\")\n",
    "add_has(\"Address Line 1\", \"has_address\")\n",
    "add_has(\"City\", \"has_city\")\n",
    "add_has(\"State\", \"has_state\")\n",
    "add_has(\"State Or Province Abbreviation\", \"has_state_abbrev\")\n",
    "add_has(\"Postal Code\", \"has_postal_code\")\n",
    "add_has(\"Country\", \"has_country\")\n",
    "add_has(\"Region\", \"has_region\")\n",
    "\n",
    "# Company name (column appears labeled as Company Sites in this dataset)\n",
    "add_has(\"Company Sites\", \"has_company_name\")\n",
    "\n",
    "# Ownership / structure\n",
    "add_has(\"Parent Company\", \"has_parent\")\n",
    "add_has(\"Global Ultimate Company\", \"has_global_ultimate\")\n",
    "add_has(\"Domestic Ultimate Company\", \"has_domestic_ultimate\")\n",
    "\n",
    "# Verifiability extras\n",
    "add_has(\"Ticker\", \"has_ticker\")\n",
    "add_has(\"Registration Number\", \"has_registration_number\")\n",
    "add_has(\"Company Description\", \"has_company_description\")\n",
    "add_has(\"Legal Status\", \"has_legal_status\")\n",
    "add_has(\"Ownership Type\", \"has_ownership_type\")\n",
    "add_has(\"Entity Type\", \"has_entity_type\")\n",
    "\n",
    "# Company status (value-coded)\n",
    "status_col = \"Company Status (Active/Inactive)\"\n",
    "if status_col in df.columns:\n",
    "    status = df[status_col].astype(\"string\").str.strip().str.lower()\n",
    "    status_map = {\"active\": 1, \"inactive\": 0}\n",
    "    features[\"company_status_binary\"] = status.map(status_map)\n",
    "    features[\"has_company_status\"] = features[\"company_status_binary\"].notna().astype(int)\n",
    "    features[\"company_status_binary\"] = features[\"company_status_binary\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121dbe6f",
   "metadata": {},
   "source": [
    "## 3) Credibility / completeness score (and missing_ratio)\n",
    "\n",
    "We include **status-known** (not status value) so inactive firms aren't penalized.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0940bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "credibility_flag_cols = [c for c in [\n",
    "    \"has_website\", \"has_address\", \"has_phone\",\n",
    "    \"has_ticker\", \"has_parent\", \"has_global_ultimate\", \"has_domestic_ultimate\",\n",
    "    \"has_registration_number\", \"has_company_description\",\n",
    "    \"has_company_status\"\n",
    "] if c in features.columns]\n",
    "\n",
    "if credibility_flag_cols:\n",
    "    features[\"credibility_score\"] = features[credibility_flag_cols].sum(axis=1)\n",
    "    features[\"credibility_score_norm\"] = features[\"credibility_score\"] / len(credibility_flag_cols)\n",
    "    features[\"missing_ratio_credibility\"] = 1 - features[\"credibility_score_norm\"]\n",
    "\n",
    "# Missingness ratios by group\n",
    "contact_cols = [\n",
    "    \"Website\", \"Phone Number\", \"Address Line 1\", \"City\", \"State\",\n",
    "    \"State Or Province Abbreviation\", \"Postal Code\", \"Country\", \"Region\"\n",
    "]\n",
    "ownership_cols = [\n",
    "    \"Parent Company\", \"Parent Country/Region\",\n",
    "    \"Global Ultimate Company\", \"Global Ultimate Country Name\",\n",
    "    \"Domestic Ultimate Company\"\n",
    "]\n",
    "financial_cols = [\n",
    "    \"Employees Single Site\", \"Employees Total\", \"Revenue (USD)\", \"Market Value (USD)\",\n",
    "    \"Corporate Family Members\", \"Year Found\"\n",
    "]\n",
    "it_cols = [\n",
    "    \"No. of PC\", \"No. of Desktops\", \"No. of Laptops\", \"No. of Routers\",\n",
    "    \"No. of Servers\", \"No. of Storage Devices\", \"IT Budget\", \"IT spend\"\n",
    "]\n",
    "code_cols = [\n",
    "    \"SIC Code\", \"8-Digit SIC Code\", \"NAICS Code\", \"NACE Rev 2 Code\",\n",
    "    \"ANZSIC Code\", \"ISIC Rev 4 Code\"\n",
    "]\n",
    "\n",
    "features[\"missing_ratio_contact\"] = missing_ratio_for(contact_cols)\n",
    "features[\"missing_ratio_ownership\"] = missing_ratio_for(ownership_cols)\n",
    "features[\"missing_ratio_financial\"] = missing_ratio_for(financial_cols)\n",
    "features[\"missing_ratio_it\"] = missing_ratio_for(it_cols)\n",
    "features[\"missing_ratio_codes\"] = missing_ratio_for(code_cols)\n",
    "\n",
    "all_missing_cols = contact_cols + ownership_cols + financial_cols + it_cols + code_cols\n",
    "features[\"missing_ratio_overall\"] = missing_ratio_for(all_missing_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e912d95",
   "metadata": {},
   "source": [
    "## 4) Organisational complexity (group size by global ultimate)\n",
    "\n",
    "Companies sharing the same global ultimate are treated as belonging to the same group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1dc1bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Global Ultimate Company\" in df.columns:\n",
    "    key = normalize_text(df[\"Global Ultimate Company\"])\n",
    "    group_sizes = key.groupby(key).transform(\"size\")\n",
    "    features[\"org_complexity_count\"] = group_sizes.fillna(0).astype(int)\n",
    "    features[\"log_org_complexity_count\"] = np.log1p(features[\"org_complexity_count\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e50d35",
   "metadata": {},
   "source": [
    "## 5) Scale + market signals\n",
    "\n",
    "These control for size so clustering isn't just \"big vs small\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47045713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coerce numeric fields and add log versions\n",
    "numeric_cols = {\n",
    "    \"Employees Total\": \"employees_total\",\n",
    "    \"Employees Single Site\": \"employees_single_site\",\n",
    "    \"Revenue (USD)\": \"revenue_usd\",\n",
    "    \"Market Value (USD)\": \"market_value_usd\",\n",
    "    \"Corporate Family Members\": \"corporate_family_members\",\n",
    "}\n",
    "\n",
    "for col, name in numeric_cols.items():\n",
    "    if col in df.columns:\n",
    "        add_numeric_feature(features, df[col], name, log=True)\n",
    "\n",
    "# Company age\n",
    "CURRENT_YEAR = 2026\n",
    "if \"Year Found\" in df.columns:\n",
    "    year_found = safe_to_numeric(df[\"Year Found\"])\n",
    "    company_age = CURRENT_YEAR - year_found\n",
    "    company_age = company_age.where((company_age >= 0) & (company_age <= 300))\n",
    "    features[\"company_age_missing\"] = company_age.isna().astype(int)\n",
    "    fill_value = company_age.median()\n",
    "    if pd.isna(fill_value):\n",
    "        fill_value = 0\n",
    "    company_age_filled = company_age.fillna(fill_value)\n",
    "    features[\"company_age\"] = company_age_filled\n",
    "    features[\"log_company_age\"] = np.log1p(company_age_filled.clip(lower=0))\n",
    "\n",
    "# Ratios\n",
    "if \"employees_total\" in features.columns and \"employees_single_site\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"employee_concentration\"] = (features[\"employees_single_site\"] / denom).fillna(0)\n",
    "\n",
    "if \"revenue_usd\" in features.columns and \"employees_total\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"revenue_per_employee\"] = (features[\"revenue_usd\"] / denom).fillna(0)\n",
    "\n",
    "if \"market_value_usd\" in features.columns and \"employees_total\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"market_value_per_employee\"] = (features[\"market_value_usd\"] / denom).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3332bd",
   "metadata": {},
   "source": [
    "## 6) Geography + multinational heuristics\n",
    "\n",
    "We avoid one-hot encoding high-cardinality city names. Use country/region + parent/ultimate country comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "105bf6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates\n",
    "if \"Lattitude\" in df.columns:\n",
    "    add_numeric_feature(features, df[\"Lattitude\"], \"latitude\", log=False)\n",
    "\n",
    "if \"Longitude\" in df.columns:\n",
    "    add_numeric_feature(features, df[\"Longitude\"], \"longitude\", log=False)\n",
    "\n",
    "entity_country = normalize_text(df[\"Country\"]) if \"Country\" in df.columns else pd.Series(pd.NA, index=df.index)\n",
    "parent_country = normalize_text(df[\"Parent Country/Region\"]) if \"Parent Country/Region\" in df.columns else pd.Series(pd.NA, index=df.index)\n",
    "global_country = normalize_text(df[\"Global Ultimate Country Name\"]) if \"Global Ultimate Country Name\" in df.columns else pd.Series(pd.NA, index=df.index)\n",
    "\n",
    "parent_present = has_value(df[\"Parent Company\"]) if \"Parent Company\" in df.columns else pd.Series(False, index=df.index)\n",
    "global_present = has_value(df[\"Global Ultimate Company\"]) if \"Global Ultimate Company\" in df.columns else pd.Series(False, index=df.index)\n",
    "\n",
    "features[\"parent_foreign_flag\"] = (parent_present & parent_country.notna() & (parent_country != entity_country)).astype(int)\n",
    "features[\"global_ultimate_foreign_flag\"] = (global_present & global_country.notna() & (global_country != entity_country)).astype(int)\n",
    "features[\"multinational_flag\"] = ((features[\"parent_foreign_flag\"] == 1) | (features[\"global_ultimate_foreign_flag\"] == 1)).astype(int)\n",
    "\n",
    "countries_df = pd.concat([entity_country, parent_country, global_country], axis=1)\n",
    "features[\"num_countries_reported\"] = countries_df.nunique(axis=1, dropna=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14adc695",
   "metadata": {},
   "source": [
    "## 7) IT / operational footprint signals\n",
    "\n",
    "We parse IT asset ranges into midpoints, then build intensity & composition signals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "175e80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_range_or_numeric(value):\n",
    "    if isinstance(value, (int, float)) and not pd.isna(value):\n",
    "        return float(value)\n",
    "    if isinstance(value, str):\n",
    "        mid = range_to_midpoint(value)\n",
    "        if not pd.isna(mid):\n",
    "            return mid\n",
    "        try:\n",
    "            return float(value.strip())\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "\n",
    "asset_cols_map = {\n",
    "    \"No. of PC\": \"pc_midpoint\",\n",
    "    \"No. of Desktops\": \"desktops_midpoint\",\n",
    "    \"No. of Laptops\": \"laptops_midpoint\",\n",
    "    \"No. of Routers\": \"routers_midpoint\",\n",
    "    \"No. of Servers\": \"servers_midpoint\",\n",
    "    \"No. of Storage Devices\": \"storage_devices_midpoint\",\n",
    "}\n",
    "\n",
    "for col, name in asset_cols_map.items():\n",
    "    if col in df.columns:\n",
    "        series = df[col].apply(parse_range_or_numeric)\n",
    "        add_numeric_feature(features, series, name, log=True)\n",
    "\n",
    "asset_feature_cols = [name for name in asset_cols_map.values() if name in features.columns]\n",
    "if asset_feature_cols:\n",
    "    features[\"it_assets_total\"] = features[asset_feature_cols].sum(axis=1)\n",
    "    features[\"log_it_assets_total\"] = np.log1p(features[\"it_assets_total\"])\n",
    "\n",
    "if \"IT Budget\" in df.columns:\n",
    "    add_numeric_feature(features, df[\"IT Budget\"], \"it_budget\", log=True)\n",
    "if \"IT spend\" in df.columns:\n",
    "    add_numeric_feature(features, df[\"IT spend\"], \"it_spend\", log=True)\n",
    "\n",
    "if \"it_budget\" in features.columns and \"it_spend\" in features.columns:\n",
    "    denom = features[\"it_budget\"].replace(0, np.nan)\n",
    "    features[\"it_spend_rate\"] = (features[\"it_spend\"] / denom).fillna(0).clip(lower=0, upper=3)\n",
    "    features[\"it_budget_gap\"] = features[\"it_budget\"] - features[\"it_spend\"]\n",
    "    features[\"log_abs_it_budget_gap\"] = np.log1p(features[\"it_budget_gap\"].abs())\n",
    "\n",
    "if \"it_assets_total\" in features.columns and \"employees_total\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"it_assets_per_employee\"] = (features[\"it_assets_total\"] / denom).fillna(0)\n",
    "\n",
    "if \"it_spend\" in features.columns and \"employees_total\" in features.columns:\n",
    "    denom = features[\"employees_total\"].replace(0, np.nan)\n",
    "    features[\"it_spend_per_employee\"] = (features[\"it_spend\"] / denom).fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed800dd",
   "metadata": {},
   "source": [
    "## 8) Industry code features (low-cardinality sector buckets)\n",
    "\n",
    "Full industry codes can be high-cardinality. For clustering/PCA, we use **2-digit buckets** (industry sectors).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8766c744",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_53240\\650323838.py:18: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[\"registration_number_count\"] = reg_counts\n"
     ]
    }
   ],
   "source": [
    "code_cols = {\n",
    "    \"SIC Code\": \"sic_code\",\n",
    "    \"8-Digit SIC Code\": \"sic8_code\",\n",
    "    \"NAICS Code\": \"naics_code\",\n",
    "    \"NACE Rev 2 Code\": \"nace2_code\",\n",
    "    \"ANZSIC Code\": \"anzsic_code\",\n",
    "    \"ISIC Rev 4 Code\": \"isic4_code\",\n",
    "}\n",
    "\n",
    "for col, prefix in code_cols.items():\n",
    "    if col in df.columns:\n",
    "        counts = df[col].astype(\"string\").apply(count_delimited_items)\n",
    "        features[f\"{prefix}_count\"] = counts\n",
    "        features[f\"has_{prefix}\"] = (counts > 0).astype(int)\n",
    "\n",
    "if \"Registration Number\" in df.columns:\n",
    "    reg_counts = df[\"Registration Number\"].astype(\"string\").apply(count_delimited_items)\n",
    "    features[\"registration_number_count\"] = reg_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b925e2b3",
   "metadata": {},
   "source": [
    "## 9) Categorical encoding (PCA/clustering ready)\n",
    "\n",
    "We one-hot encode selected low-cardinality categoricals + sector buckets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f1af319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical columns encoded: ['Region', 'Entity Type', 'Ownership Type', 'Legal Status', 'Franchise Status', 'Manufacturing Status', 'Registration Number Type']\n",
      "One-hot shape: (8559, 31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_53240\\3519656746.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[name] = mapped.fillna(0).astype(int)\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_53240\\3519656746.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f\"{name}_missing\"] = mapped.isna().astype(int)\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_53240\\3519656746.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[name] = mapped.fillna(0).astype(int)\n",
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_53240\\3519656746.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  features[f\"{name}_missing\"] = mapped.isna().astype(int)\n"
     ]
    }
   ],
   "source": [
    "bool_map = {\"true\": 1, \"false\": 0, \"yes\": 1, \"no\": 0, \"y\": 1, \"n\": 0, \"1\": 1, \"0\": 0}\n",
    "\n",
    "def add_boolean_feature(col, name):\n",
    "    if col not in df.columns:\n",
    "        return\n",
    "    s = df[col].astype(\"string\").str.strip().str.lower()\n",
    "    mapped = s.map(bool_map)\n",
    "    features[name] = mapped.fillna(0).astype(int)\n",
    "    features[f\"{name}_missing\"] = mapped.isna().astype(int)\n",
    "\n",
    "add_boolean_feature(\"Is Headquarters\", \"is_headquarters\")\n",
    "add_boolean_feature(\"Is Domestic Ultimate\", \"is_domestic_ultimate\")\n",
    "\n",
    "candidate_categoricals = [\n",
    "    \"Region\",\n",
    "    \"Entity Type\",\n",
    "    \"Ownership Type\",\n",
    "    \"Legal Status\",\n",
    "    \"Franchise Status\",\n",
    "    \"Manufacturing Status\",\n",
    "    \"Registration Number Type\",\n",
    "]\n",
    "\n",
    "categorical_cols = []\n",
    "for col in candidate_categoricals:\n",
    "    if col in df.columns and df[col].nunique(dropna=True) <= 20:\n",
    "        categorical_cols.append(col)\n",
    "\n",
    "if categorical_cols:\n",
    "    df_cats = df[categorical_cols].copy()\n",
    "    for col in categorical_cols:\n",
    "        df_cats[col] = df_cats[col].astype(\"string\").str.strip()\n",
    "    df_dummies = pd.get_dummies(df_cats, prefix=categorical_cols, prefix_sep=\"__\", dummy_na=True)\n",
    "else:\n",
    "    df_dummies = pd.DataFrame(index=df.index)\n",
    "\n",
    "print(\"Categorical columns encoded:\", categorical_cols)\n",
    "print(\"One-hot shape:\", df_dummies.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e3a25",
   "metadata": {},
   "source": [
    "## 10) Build final feature matrices (raw + scaled)\n",
    "\n",
    "- Drop obvious identifiers & free-text\n",
    "- Keep numeric + engineered features + one-hot columns\n",
    "- Create scaled matrix for PCA/clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23d4479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape (pre-impute): (8559, 140)\n"
     ]
    }
   ],
   "source": [
    "df_features_raw = pd.concat([features, df_dummies], axis=1)\n",
    "\n",
    "# Drop columns that are fully missing\n",
    "df_features_raw = df_features_raw.dropna(axis=1, how=\"all\")\n",
    "\n",
    "# Ensure numeric-only\n",
    "df_features_raw = df_features_raw.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "print(\"Feature matrix shape (pre-impute):\", df_features_raw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1089a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix any remaining NaNs\n",
    "if df_features_raw.isna().any().any():\n",
    "    nan_cols = df_features_raw.columns[df_features_raw.isna().any()].tolist()\n",
    "    print(\"NaNs found in raw features. Top columns:\", nan_cols[:20])\n",
    "    for col in nan_cols:\n",
    "        series = df_features_raw[col]\n",
    "        fill_value = series.median()\n",
    "        if pd.isna(fill_value):\n",
    "            fill_value = 0\n",
    "        df_features_raw[col] = series.fillna(fill_value)\n",
    "\n",
    "assert not df_features_raw.isna().any().any(), \"NaNs remain in raw features\"\n",
    "\n",
    "# Scale for PCA/clustering\n",
    "scaler = StandardScaler()\n",
    "df_features_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_features_raw),\n",
    "    columns=df_features_raw.columns,\n",
    "    index=df_features_raw.index\n",
    ")\n",
    "\n",
    "assert not df_features_scaled.isna().any().any(), \"NaNs remain in scaled features\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f31cbb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row id map columns: ['row_index', 'DUNS Number', 'Company Name']\n"
     ]
    }
   ],
   "source": [
    "def find_first_column(candidates):\n",
    "    for col in candidates:\n",
    "        if col in df.columns:\n",
    "            return col\n",
    "    return None\n",
    "\n",
    "duns_col = None\n",
    "for col in df.columns:\n",
    "    if col.strip().lower() == \"duns number\":\n",
    "        duns_col = col\n",
    "        break\n",
    "\n",
    "name_col = find_first_column([\"Company Name\", \"Company\", \"Company Sites\"])\n",
    "\n",
    "row_id_map = pd.DataFrame({\"row_index\": df.index})\n",
    "if duns_col:\n",
    "    row_id_map[\"DUNS Number\"] = df[duns_col]\n",
    "if name_col:\n",
    "    row_id_map[\"Company Name\"] = df[name_col]\n",
    "\n",
    "print(\"Row id map columns:\", row_id_map.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b8d51cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = {\n",
    "    \"has_website\": \"1 if Website is present\",\n",
    "    \"has_phone\": \"1 if Phone Number is present\",\n",
    "    \"has_address\": \"1 if Address Line 1 is present\",\n",
    "    \"has_city\": \"1 if City is present\",\n",
    "    \"has_state\": \"1 if State is present\",\n",
    "    \"has_state_abbrev\": \"1 if State Or Province Abbreviation is present\",\n",
    "    \"has_postal_code\": \"1 if Postal Code is present\",\n",
    "    \"has_country\": \"1 if Country is present\",\n",
    "    \"has_region\": \"1 if Region is present\",\n",
    "    \"has_company_name\": \"1 if Company Sites (company name) is present\",\n",
    "    \"has_parent\": \"1 if Parent Company is present\",\n",
    "    \"has_global_ultimate\": \"1 if Global Ultimate Company is present\",\n",
    "    \"has_domestic_ultimate\": \"1 if Domestic Ultimate Company is present\",\n",
    "    \"has_ticker\": \"1 if Ticker is present\",\n",
    "    \"has_registration_number\": \"1 if Registration Number is present\",\n",
    "    \"has_company_description\": \"1 if Company Description is present\",\n",
    "    \"has_legal_status\": \"1 if Legal Status is present\",\n",
    "    \"has_ownership_type\": \"1 if Ownership Type is present\",\n",
    "    \"has_entity_type\": \"1 if Entity Type is present\",\n",
    "    \"company_status_binary\": \"1 if Company Status is active, 0 if inactive or missing\",\n",
    "    \"has_company_status\": \"1 if Company Status is present\",\n",
    "    \"credibility_score\": \"Count of core transparency/ownership flags present\",\n",
    "    \"credibility_score_norm\": \"Credibility score normalized by number of flags\",\n",
    "    \"missing_ratio_credibility\": \"1 - credibility_score_norm\",\n",
    "    \"missing_ratio_contact\": \"Missing ratio across contact fields\",\n",
    "    \"missing_ratio_ownership\": \"Missing ratio across ownership fields\",\n",
    "    \"missing_ratio_financial\": \"Missing ratio across financial fields\",\n",
    "    \"missing_ratio_it\": \"Missing ratio across IT fields\",\n",
    "    \"missing_ratio_codes\": \"Missing ratio across industry code fields\",\n",
    "    \"missing_ratio_overall\": \"Missing ratio across key groups\",\n",
    "    \"org_complexity_count\": \"Count of entities sharing the same global ultimate\",\n",
    "    \"log_org_complexity_count\": \"log1p of org_complexity_count\",\n",
    "    \"employees_total\": \"Employees total (imputed)\",\n",
    "    \"employees_single_site\": \"Employees single site (imputed)\",\n",
    "    \"revenue_usd\": \"Revenue USD (imputed)\",\n",
    "    \"market_value_usd\": \"Market value USD (imputed)\",\n",
    "    \"corporate_family_members\": \"Corporate family members (imputed)\",\n",
    "    \"company_age\": \"Company age in years (imputed)\",\n",
    "    \"employee_concentration\": \"Employees single site divided by employees total\",\n",
    "    \"revenue_per_employee\": \"Revenue per employee (USD)\",\n",
    "    \"market_value_per_employee\": \"Market value per employee (USD)\",\n",
    "    \"latitude\": \"Latitude (imputed)\",\n",
    "    \"longitude\": \"Longitude (imputed)\",\n",
    "    \"parent_foreign_flag\": \"1 if parent country differs from entity country\",\n",
    "    \"global_ultimate_foreign_flag\": \"1 if global ultimate country differs from entity country\",\n",
    "    \"multinational_flag\": \"1 if parent or global ultimate is foreign\",\n",
    "    \"num_countries_reported\": \"Count of unique countries reported (entity/parent/global)\",\n",
    "    \"pc_midpoint\": \"Midpoint of PC count range (imputed)\",\n",
    "    \"desktops_midpoint\": \"Midpoint of desktop count range (imputed)\",\n",
    "    \"laptops_midpoint\": \"Midpoint of laptop count range (imputed)\",\n",
    "    \"routers_midpoint\": \"Midpoint of router count range (imputed)\",\n",
    "    \"servers_midpoint\": \"Midpoint of server count range (imputed)\",\n",
    "    \"storage_devices_midpoint\": \"Midpoint of storage device count range (imputed)\",\n",
    "    \"it_assets_total\": \"Sum of IT asset midpoints\",\n",
    "    \"log_it_assets_total\": \"log1p of it_assets_total\",\n",
    "    \"it_budget\": \"IT budget (imputed)\",\n",
    "    \"it_spend\": \"IT spend (imputed)\",\n",
    "    \"it_spend_rate\": \"IT spend divided by IT budget (clipped)\",\n",
    "    \"it_budget_gap\": \"IT budget minus IT spend\",\n",
    "    \"log_abs_it_budget_gap\": \"log1p of absolute IT budget gap\",\n",
    "    \"it_assets_per_employee\": \"IT assets per employee\",\n",
    "    \"it_spend_per_employee\": \"IT spend per employee\",\n",
    "    \"sic_code_count\": \"Number of SIC codes (split on delimiters)\",\n",
    "    \"sic8_code_count\": \"Number of 8-digit SIC codes (split on delimiters)\",\n",
    "    \"naics_code_count\": \"Number of NAICS codes (split on delimiters)\",\n",
    "    \"nace2_code_count\": \"Number of NACE Rev 2 codes (split on delimiters)\",\n",
    "    \"anzsic_code_count\": \"Number of ANZSIC codes (split on delimiters)\",\n",
    "    \"isic4_code_count\": \"Number of ISIC Rev 4 codes (split on delimiters)\",\n",
    "    \"has_sic_code\": \"1 if SIC Code is present\",\n",
    "    \"has_sic8_code\": \"1 if 8-digit SIC Code is present\",\n",
    "    \"has_naics_code\": \"1 if NAICS Code is present\",\n",
    "    \"has_nace2_code\": \"1 if NACE Rev 2 Code is present\",\n",
    "    \"has_anzsic_code\": \"1 if ANZSIC Code is present\",\n",
    "    \"has_isic4_code\": \"1 if ISIC Rev 4 Code is present\",\n",
    "    \"registration_number_count\": \"Number of registration numbers (split on delimiters)\",\n",
    "}\n",
    "\n",
    "def describe_feature(name):\n",
    "    if name in feature_descriptions:\n",
    "        return feature_descriptions[name]\n",
    "    if name.endswith(\"_missing\"):\n",
    "        base = name[:-8]\n",
    "        return f\"Missing indicator for {base}\"\n",
    "    if name.startswith(\"log_\"):\n",
    "        base = name[4:]\n",
    "        return f\"log1p of {base}\"\n",
    "    if \"__\" in name:\n",
    "        base, val = name.split(\"__\", 1)\n",
    "        val_norm = val.strip().lower()\n",
    "        if val_norm in (\"nan\", \"<na>\"):\n",
    "            return f\"Missing indicator for {base}\"\n",
    "        return f\"One-hot: {base} = {val}\"\n",
    "    return \"Derived numeric feature\"\n",
    "\n",
    "feature_dict = pd.DataFrame({\n",
    "    \"feature\": df_features_raw.columns,\n",
    "    \"description\": [describe_feature(c) for c in df_features_raw.columns],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf79ad1",
   "metadata": {},
   "source": [
    "## 11) Export for Member 3 (optional)\n",
    "\n",
    "Uncomment to export once Member 1's clean_base.csv is ready.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0c298ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: c:\\Users\\ADMIN\\sdsdatathon\\data\\processed\\features_for_clustering_raw.csv\n",
      "Saved: c:\\Users\\ADMIN\\sdsdatathon\\data\\processed\\features_for_clustering_scaled.csv\n",
      "Saved: c:\\Users\\ADMIN\\sdsdatathon\\data\\processed\\row_id_map.csv\n",
      "Saved: c:\\Users\\ADMIN\\sdsdatathon\\docs\\feature_dictionary_member2.csv\n",
      "clean_base shape: (8559, 72)\n",
      "Raw features shape: (8559, 140)\n",
      "Scaled features shape: (8559, 140)\n",
      "Any NaNs in raw features? False\n",
      "Any NaNs in scaled features? False\n",
      "Number of feature columns: 140\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = DEST_DIR\n",
    "DOCS_DIR = ROOT / \"docs\"\n",
    "DOCS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "features_raw_path = OUTPUT_DIR / \"features_for_clustering_raw.csv\"\n",
    "features_scaled_path = OUTPUT_DIR / \"features_for_clustering_scaled.csv\"\n",
    "row_id_map_path = OUTPUT_DIR / \"row_id_map.csv\"\n",
    "feature_dict_path = DOCS_DIR / \"feature_dictionary_member2.csv\"\n",
    "\n",
    "df_features_raw.to_csv(features_raw_path, index=False)\n",
    "df_features_scaled.to_csv(features_scaled_path, index=False)\n",
    "row_id_map.to_csv(row_id_map_path, index=False)\n",
    "feature_dict.to_csv(feature_dict_path, index=False)\n",
    "\n",
    "print(\"Saved:\", features_raw_path)\n",
    "print(\"Saved:\", features_scaled_path)\n",
    "print(\"Saved:\", row_id_map_path)\n",
    "print(\"Saved:\", feature_dict_path)\n",
    "\n",
    "print(\"clean_base shape:\", df.shape)\n",
    "print(\"Raw features shape:\", df_features_raw.shape)\n",
    "print(\"Scaled features shape:\", df_features_scaled.shape)\n",
    "print(\"Any NaNs in raw features?\", df_features_raw.isna().any().any())\n",
    "print(\"Any NaNs in scaled features?\", df_features_scaled.isna().any().any())\n",
    "print(\"Number of feature columns:\", df_features_raw.shape[1])\n",
    "\n",
    "assert df_features_raw.shape[0] == df.shape[0]\n",
    "assert df_features_scaled.shape[0] == df.shape[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
