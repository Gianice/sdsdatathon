{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c846c96",
   "metadata": {},
   "source": [
    "# PCA + KMeans Clustering (Reproducible Pipeline)\n",
    "\n",
    "This notebook clusters companies using a pre-built, **scaled feature matrix** (for modeling) and links the resulting cluster labels back to a more readable **original dataset** (for interpretation). It produces two outputs: (1) the original dataset with a `cluster` column, and (2) a cluster profile summary computed on original features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b4e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B) Imports (consolidated)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0de91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C) Config / File Paths (centralized)\n",
    "SCALED_PATH = \"../data/processed/features_for_clustering_scaled.csv\"\n",
    "ORIGINAL_PATH = \"../data/processed/cleaned_base.csv\"\n",
    "\n",
    "OUT_ORIGINAL_WITH_CLUSTERS = \"../data/processed/original_with_clusters.csv\"\n",
    "OUT_CLUSTER_PROFILE = \"../data/processed/cluster_summary_original_features.csv\"\n",
    "\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a59dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D) Load Data (scaled features + original readable dataset)\n",
    "df_scaled = pd.read_csv(SCALED_PATH)\n",
    "df_original = pd.read_csv(ORIGINAL_PATH, low_memory=False)\n",
    "\n",
    "print(\"df_scaled shape   :\", df_scaled.shape)\n",
    "print(\"df_original shape :\", df_original.shape)\n",
    "\n",
    "# Validate row alignment (must be same row order for label linking)\n",
    "if len(df_scaled) != len(df_original):\n",
    "    raise ValueError(\n",
    "        f\"Row mismatch! df_scaled has {len(df_scaled)} rows but df_original has {len(df_original)} rows.\\n\"\n",
    "        \"They MUST represent the same entities in the same order so cluster labels can be linked back safely.\\n\"\n",
    "        \"Fix upstream preprocessing to avoid dropping/shuffling rows, or re-export the aligned files.\"\n",
    "    )\n",
    "\n",
    "# Optional stronger check if a stable row id exists in both files\n",
    "candidate_id_cols = [\"row_id\", \"RowID\", \"id\", \"ID\"]\n",
    "id_col = next((c for c in candidate_id_cols if c in df_scaled.columns and c in df_original.columns), None)\n",
    "if id_col is not None:\n",
    "    if not (df_scaled[id_col].astype(str).values == df_original[id_col].astype(str).values).all():\n",
    "        raise ValueError(\n",
    "            f\"Row order mismatch detected via column '{id_col}'.\\n\"\n",
    "            \"The two files must be aligned row-by-row before clustering labels can be joined back.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e8a2d1",
   "metadata": {},
   "source": [
    "## E) Dimensionality Reduction\n",
    "\n",
    "We reduce dimensionality to make KMeans faster and more stable.\n",
    "- If the feature matrix is **dense** (like a numeric CSV), we use **PCA** with `n_components=0.95` to keep 95% variance.\n",
    "- If the feature matrix is **sparse** (common after one-hot encoding), PCA may require centering and can become expensive; in that case we use **TruncatedSVD**, which works directly with sparse matrices without densifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E2) Compute PCA (dense) or TruncatedSVD (sparse) -> df_pca\n",
    "X = df_scaled\n",
    "\n",
    "# Detect sparse matrices without forcing a SciPy dependency (SciPy is optional here)\n",
    "is_sparse = False\n",
    "try:\n",
    "    from scipy import sparse  # optional; only used for sparse checks\n",
    "    is_sparse = sparse.issparse(X)\n",
    "except Exception:\n",
    "    is_sparse = False\n",
    "\n",
    "if is_sparse:\n",
    "    # TruncatedSVD does not support n_components as a variance ratio directly.\n",
    "    # Use a reasonable cap; increase if cumulative variance is too low for your use case.\n",
    "    n_components = min(200, X.shape[1] - 1) if X.shape[1] > 1 else 1\n",
    "    reducer = TruncatedSVD(n_components=n_components, random_state=RANDOM_STATE)\n",
    "    X_reduced = reducer.fit_transform(X)\n",
    "    explained = float(np.sum(reducer.explained_variance_ratio_))\n",
    "    reducer_name = \"TruncatedSVD\"\n",
    "else:\n",
    "    reducer = PCA(n_components=0.95, random_state=RANDOM_STATE)\n",
    "    X_reduced = reducer.fit_transform(X)\n",
    "    explained = float(np.sum(reducer.explained_variance_ratio_))\n",
    "    reducer_name = \"PCA\"\n",
    "\n",
    "df_pca = pd.DataFrame(X_reduced, columns=[f\"PC{i+1}\" for i in range(X_reduced.shape[1])])\n",
    "\n",
    "print(f\"Reducer: {reducer_name}\")\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Reduced shape :\", df_pca.shape)\n",
    "print(\"Explained variance (sum):\", explained)\n",
    "\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5c09e6",
   "metadata": {},
   "source": [
    "## F) Choose K\n",
    "\n",
    "We evaluate a small range of cluster counts ($k=2$ to $10$) using:\n",
    "- **Silhouette score** (higher is better)\n",
    "- **Inertia** (elbow method; lower is better, diminishing returns)\n",
    "\n",
    "By default, we select `best_k` from the highest silhouette score (you can override if the elbow plot suggests a different tradeoff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac165e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2) K selection: silhouette + elbow\n",
    "K_range = range(2, 11)\n",
    "sil_scores = {}\n",
    "inertias = {}\n",
    "\n",
    "n_samples = len(df_pca)\n",
    "for k in K_range:\n",
    "    if k >= n_samples:\n",
    "        break\n",
    "    km = KMeans(n_clusters=k, random_state=RANDOM_STATE, n_init=10)\n",
    "    labels = km.fit_predict(df_pca)\n",
    "    sil = silhouette_score(df_pca, labels)\n",
    "    sil_scores[k] = float(sil)\n",
    "    inertias[k] = float(km.inertia_)\n",
    "    print(f\"k={k}, silhouette={sil:.4f}, inertia={km.inertia_:.2f}\")\n",
    "\n",
    "best_k = max(sil_scores, key=sil_scores.get)\n",
    "print(\"\\nBest k by silhouette:\", best_k)\n",
    "# best_k = 5  # Uncomment to override manually\n",
    "\n",
    "# Plots (shown once)\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(list(inertias.keys()), list(inertias.values()), marker=\"o\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.title(\"Elbow Method (KMeans)\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(list(sil_scores.keys()), list(sil_scores.values()), marker=\"o\")\n",
    "plt.xlabel(\"k\")\n",
    "plt.ylabel(\"Silhouette score\")\n",
    "plt.title(\"Silhouette Score vs k\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e077df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G1) Train final KMeans\n",
    "kmeans = KMeans(n_clusters=best_k, random_state=RANDOM_STATE, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(df_pca)\n",
    "\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "print(\"Cluster counts:\")\n",
    "display(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45f162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G2) Visualize: PC1 vs PC2 colored by cluster\n",
    "if df_pca.shape[1] < 2:\n",
    "    raise ValueError(\"Need at least 2 components to plot PC1 vs PC2.\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df_pca[\"PC1\"], df_pca[\"PC2\"], c=cluster_labels, s=10)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"Clusters (PC space)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5e64a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H) Link clusters back to original data + save\n",
    "df_original_with_clusters = df_original.copy()\n",
    "df_original_with_clusters[\"cluster\"] = cluster_labels\n",
    "\n",
    "df_original_with_clusters.to_csv(OUT_ORIGINAL_WITH_CLUSTERS, index=False)\n",
    "print(\"Saved original dataset with clusters to:\", OUT_ORIGINAL_WITH_CLUSTERS)\n",
    "\n",
    "df_original_with_clusters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9674bc",
   "metadata": {},
   "source": [
    "## I) Cluster Interpretation Summary\n",
    "\n",
    "Interpretation should be done on **original features**, not on PC features. Below we build a simple `profile_display` table per cluster:\n",
    "- cluster size\n",
    "- means/medians for known numeric columns if present\n",
    "- % coverage for known flag columns if present\n",
    "- top value for known categorical columns if present\n",
    "\n",
    "The code is written to be robust to missing columns (it will only summarize what exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18644216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I2) Build cluster profile summary on original features\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "candidate_numeric = [\n",
    "    \"Employees Single Site\",\n",
    "    \"Employees Total\",\n",
    "    \"Revenue (USD)\",\n",
    "    \"Year Found\",\n",
    "    \"Market Value (USD)\"\n",
    "]\n",
    "numeric_cols = [c for c in candidate_numeric if c in df_original_with_clusters.columns]\n",
    "\n",
    "candidate_cats = [\n",
    "    \"Country\",\n",
    "    \"Region\",\n",
    "    \"Entity Type\",\n",
    "    \"Ownership Type\",\n",
    "    \"NAICS Description\",\n",
    "    \"SIC Description\",\n",
    "    \"Legal Status\"\n",
    "]\n",
    "cat_cols = [c for c in candidate_cats if c in df_original_with_clusters.columns]\n",
    "\n",
    "candidate_flags = [\"has_website\", \"has_phone\", \"has_address\", \"has_city\", \"has_country\"]\n",
    "flag_cols = [c for c in candidate_flags if c in df_original_with_clusters.columns]\n",
    "\n",
    "print(\"Summary columns used:\")\n",
    "print(\"- Numeric:\", numeric_cols)\n",
    "print(\"- Categorical:\", cat_cols)\n",
    "print(\"- Flags:\", flag_cols)\n",
    "\n",
    "def top_value(series: pd.Series):\n",
    "    s = series.dropna()\n",
    "    if len(s) == 0:\n",
    "        return None\n",
    "    return s.value_counts().index[0]\n",
    "\n",
    "profile = df_original_with_clusters.groupby(\"cluster\").agg(size=(\"cluster\", \"count\"))\n",
    "\n",
    "for col in numeric_cols:\n",
    "    profile[f\"{col}__mean\"] = df_original_with_clusters.groupby(\"cluster\")[col].mean()\n",
    "    profile[f\"{col}__median\"] = df_original_with_clusters.groupby(\"cluster\")[col].median()\n",
    "\n",
    "for col in flag_cols:\n",
    "    profile[f\"{col}__pct\"] = df_original_with_clusters.groupby(\"cluster\")[col].mean() * 100\n",
    "\n",
    "for col in cat_cols:\n",
    "    profile[f\"{col}__top\"] = df_original_with_clusters.groupby(\"cluster\")[col].apply(top_value)\n",
    "\n",
    "needed_for_risk = [\"has_website\", \"has_phone\", \"has_address\"]\n",
    "if all(c in df_original_with_clusters.columns for c in needed_for_risk):\n",
    "    df_original_with_clusters[\"risk_score\"] = (\n",
    "        (1 - df_original_with_clusters[\"has_website\"]) +\n",
    "        (1 - df_original_with_clusters[\"has_phone\"]) +\n",
    "        (1 - df_original_with_clusters[\"has_address\"])\n",
    "    )\n",
    "    profile[\"risk_score__mean\"] = df_original_with_clusters.groupby(\"cluster\")[\"risk_score\"].mean()\n",
    "\n",
    "profile = profile.sort_values(\"size\", ascending=False)\n",
    "\n",
    "profile_display = profile.copy()\n",
    "for c in profile_display.columns:\n",
    "    if \"__mean\" in c or \"__median\" in c:\n",
    "        profile_display[c] = profile_display[c].round(2)\n",
    "    if \"__pct\" in c:\n",
    "        profile_display[c] = profile_display[c].round(1)\n",
    "    if \"risk_score\" in c:\n",
    "        profile_display[c] = profile_display[c].round(3)\n",
    "\n",
    "profile_display.to_csv(OUT_CLUSTER_PROFILE, index=True)\n",
    "print(\"Saved cluster summary to:\", OUT_CLUSTER_PROFILE)\n",
    "\n",
    "display(profile_display)\n",
    "\n",
    "# Clean report view (subset)\n",
    "important_cols = [\"size\"]\n",
    "for col in [\"Revenue (USD)__mean\", \"Revenue (USD)__median\", \"Employees Total__mean\", \"Employees Total__median\"]:\n",
    "    if col in profile_display.columns:\n",
    "        important_cols.append(col)\n",
    "for col in flag_cols:\n",
    "    colname = f\"{col}__pct\"\n",
    "    if colname in profile_display.columns:\n",
    "        important_cols.append(colname)\n",
    "for col in [\"Country__top\", \"NAICS Description__top\", \"Ownership Type__top\", \"Entity Type__top\"]:\n",
    "    if col in profile_display.columns:\n",
    "        important_cols.append(col)\n",
    "if \"risk_score__mean\" in profile_display.columns:\n",
    "    important_cols.append(\"risk_score__mean\")\n",
    "\n",
    "display(profile_display[important_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492b5579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# J) (Optional) Inspect a single cluster\n",
    "CLUSTER_TO_VIEW = 0  # change as needed\n",
    "\n",
    "# Example: show 20 sample rows from the chosen cluster\n",
    "df_original_with_clusters[df_original_with_clusters[\"cluster\"] == CLUSTER_TO_VIEW].head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
